This works of mapping and localization with aruco markers uses two steps. Below is the explanation of how to map.

You will need a turtlebot with a kinect camera + ROS(you also can simulate with gazebo)

$ roscore(start ros)
$ roslaunch turtlebot_bringup minimal.launch(bringup turtlebot)
$ roslaunch turtlebot_teleop keyboard_teleop.launch(to move turtlebot)
$ roslaunch turtlebot_navigation gmapping_demo.launch(begin mapping,this launch also launchs kinect)
$ rosrun map_server map_saver -f /tmp/my_map(for save the map when the map is finished, /tmp/my_map



####Motion estimator ros

motion_estimator.cpp will track points and show how the tracked point moved in space.

Motion estimator needs two ros topics to work, /camera/rgb/image_raw and /camera/depth/image_raw,
if your bag has rgb and depth image but not those topics: E.g /camera/rgb/image_color and /camera/depth/image use rosbag play --clock camera/rgb/image_color:=/camera/rgb/image_raw camera/depth/image:=camera/depth/image_raw "bag".bag

####Bag Loader

bag_loader.cpp will load and show in window a rgb and a depth topic

#### Goal

goal.cpp will send a goal to turtlebot moves to a x,y position

#### Keyboard Input

Keyboard_input.cpp save all markers id and positions that were saw in any frame when mapping, and in localization sends a goal to move to a marker id.

#### Listening keyboard

listening_Keyboard.cpp listen to a string

 


turtlebot minimal
ligar rviz
fazer um mapa com gmapping.demo e salvar para usar no amcl.demo
abrir o mapa no amcl.demo, e usar o rviz para estimar a pose do robô, ou modificar o launch.demo para a posição inicial do robô
usar goal.cpp para mandar o robô pro lugar que vc quer 

